{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Lecture-3 of Short Course of Temporal Point Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11100cb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed \n",
    "np.random.seed(12345)\n",
    "torch.random.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Hawkes Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-time LSTM cell\n",
    "\n",
    "The LSTM cell $c(t)$ drifts from $c_{\\text{start}}$ towards $c_{\\text{target}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTLSTMCell(nn.Module): \n",
    "    \n",
    "    def __init__(self, hdim): \n",
    "        super(CTLSTMCell, self).__init__()\n",
    "        \"\"\"\n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.hdim = hdim \n",
    "        self.lin = nn.Linear(hdim*2, hdim*6, bias=True)\n",
    "        return \n",
    "    \n",
    "    def forward(self, x, h, c, ct): \n",
    "        \"\"\"\n",
    "        x : input embedding\n",
    "        h : hidden state right before time t \n",
    "        c : LSTM cell right before time t \n",
    "        ct : LSTM target cell given current history\n",
    "        \"\"\"\n",
    "        x = torch.cat((x, h), dim=0)\n",
    "        y = self.lin(x)\n",
    "        \n",
    "        gi, gf, z, git, gft, gd = y.chunk(6, 0)\n",
    "        \n",
    "        gi = torch.sigmoid(gi)\n",
    "        gf = torch.sigmoid(gf)\n",
    "        z = torch.tanh(z)\n",
    "        git = torch.sigmoid(git)\n",
    "        gft = torch.sigmoid(gft)\n",
    "        gd = F.softplus(gd)\n",
    "        \n",
    "        cs = gf * c + gi * z \n",
    "        ct = gft * ct + git * z\n",
    "        \n",
    "        return cs, ct, gd\n",
    "    \n",
    "    def decay(self, cs, ct, gd, dt): \n",
    "        \"\"\"\n",
    "        cs : LSTM start cell\n",
    "        ct : LSTM target cell \n",
    "        gd : decay gate\n",
    "        dt : elapsed time \n",
    "        \"\"\"\n",
    "        c = ct + (cs - ct) * torch.exp(-gd * dt)\n",
    "        h = torch.tanh(c)\n",
    "        \n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Hawkes process\n",
    "\n",
    "The intensity is defined as $\\lambda_k(t) = \\text{Softplus}(\\text{Linear}(h(t)))$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHP(nn.Module): \n",
    "    \n",
    "    def __init__(self, kdim, hdim): \n",
    "        super(NHP, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.eps = np.finfo(float).eps \n",
    "        self.max = np.finfo(float).max \n",
    "        self.kdim = kdim \n",
    "        self.hdim = hdim \n",
    "        self.BOS = kdim \n",
    "        \n",
    "        self.emb_in = nn.Embedding(kdim+1, hdim)\n",
    "        self.ctlstm = CTLSTMCell(hdim)\n",
    "        self.emb_out = nn.Linear(hdim, kdim)\n",
    "        \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.update(self.BOS, 0.0)\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        x = self.emb_in(torch.LongTensor([k]))[0]\n",
    "        self.cs, self.ct, self.gd = self.ctlstm(x, h, c, self.ct)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        return F.softplus(self.emb_out(h))\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        intensities = self.compute_intensities(dt)\n",
    "        return torch.sum(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sequence of events by thinning algorithm\n",
    "\n",
    "For the code to be easy to understand, I only have non-vectorized implementation. Please check the repos for my published papers for highly vectorized and optimized implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinning(model): \n",
    "    dt = 0.0\n",
    "    bound = 100.0 \n",
    "    # manualy chosen for simplicity\n",
    "    # in principle, it can be found using the method in Appendix B.3 of Mei & Eisner 2017\n",
    "    while True: \n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        dt += -np.log(1-u) / bound\n",
    "        intens = model.compute_intensities(dt)\n",
    "        total_inten = torch.sum(intens)\n",
    "        accept_prob = total_inten / bound\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        if u <= accept_prob: \n",
    "            break \n",
    "    \n",
    "    k = torch.multinomial(intens, 1)\n",
    "    \n",
    "    return k, dt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw data from a low-entropy distribution: (1) draw $dt$ from a univariate NHP; (2) draw $k$ from a n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over time interval [0, 100.0]\n",
      "# of events : 78\n"
     ]
    }
   ],
   "source": [
    "kdim = 32 \n",
    "hdim = 8\n",
    "nhp = NHP(1, hdim)\n",
    "# init by BOS \n",
    "nhp.start()\n",
    "\n",
    "T = 100.0\n",
    "t = 0\n",
    "seq = []\n",
    "CONTEXT = 0\n",
    "\n",
    "while True:\n",
    "    # draw dt using thinning algorithm\n",
    "    \n",
    "    _, dt = thinning(nhp)\n",
    "    t += dt\n",
    "    if t <= T: \n",
    "        k = (CONTEXT + 1) % kdim\n",
    "        seq += [(dt, k)] # track dt, not t, easy to use\n",
    "        # update model \n",
    "        nhp.forward(0, dt)\n",
    "        # update CONTEXT \n",
    "        CONTEXT = k\n",
    "    else: \n",
    "        break\n",
    "\n",
    "print(f\"over time interval [0, {T}]\")\n",
    "print(f\"# of events : {len(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(data, model): \n",
    "    \"\"\"\n",
    "    compute log-likelihood of seq under model\n",
    "    \"\"\"\n",
    "    J = 10\n",
    "    model.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    for event in seq: \n",
    "        dt, k = event\n",
    "        # log intensity \n",
    "        loglik += torch.log(model.compute_intensities(dt)[k])\n",
    "        # integral\n",
    "        integral = 0.0 \n",
    "        for j in range(J): \n",
    "            # draw uniform-distributed time points\n",
    "            dtj = np.random.uniform(0.0, dt)\n",
    "            integral += model.compute_total_intensity(dtj)\n",
    "        integral /= J \n",
    "        integral *= dt \n",
    "        loglik -= integral\n",
    "        # update model\n",
    "        model.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2213.8076; time cost = 0.2930\n",
      "Iter-1: log-likelihood = -2118.6753; time cost = 0.2883\n",
      "Iter-2: log-likelihood = -1947.0254; time cost = 0.2890\n",
      "Iter-3: log-likelihood = -1701.9241; time cost = 0.2909\n",
      "Iter-4: log-likelihood = -1387.6753; time cost = 0.2914\n",
      "Iter-5: log-likelihood = -1055.1245; time cost = 0.2903\n",
      "Iter-6: log-likelihood = -770.9929; time cost = 0.2888\n",
      "Iter-7: log-likelihood = -568.3903; time cost = 0.2893\n",
      "Iter-8: log-likelihood = -449.9092; time cost = 0.2909\n",
      "Iter-9: log-likelihood = -391.9956; time cost = 0.2875\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "model = NHP(kdim, hdim)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "MAX_ITER = 10\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = mle(seq, model)\n",
    "    loss = -loglik \n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNoise(nn.Module): \n",
    "    \"\"\"\n",
    "    a simple noise distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kdim, total_intensity): \n",
    "        super(SimpleNoise, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.total_intensity = total_intensity\n",
    "        self.inten = total_intensity / kdim\n",
    "        self.intens = torch.zeros([kdim], dtype = torch.float32).fill_(self.inten)\n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        # do nothing\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        # does nothing\n",
    "        # simple distribution, no dependence on history\n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        return self.intens\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        return self.total_intensity\n",
    "    \n",
    "    def draw(self, dt): \n",
    "        noise_events = []\n",
    "        dtj = 0.0\n",
    "        while True: \n",
    "            # draw noise time\n",
    "            u = np.random.uniform(0.0, 1.0)\n",
    "            dtj += -np.log(1-u) / self.total_intensity\n",
    "            if dtj <= dt: \n",
    "                # draw noise type\n",
    "                kj = torch.multinomial(self.intens, 1)[0]\n",
    "                noise_events += [(dtj, kj)]\n",
    "            else: \n",
    "                break \n",
    "        return noise_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce(data, model, noise): \n",
    "    \"\"\"\n",
    "    compute log-likelihood of seq under model\n",
    "    \"\"\"\n",
    "    model.start()\n",
    "    noise.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    for event in seq: \n",
    "        dt, k = event\n",
    "        \n",
    "        # real event & noise non-event\n",
    "        p_real = model.compute_intensities(dt)[k]\n",
    "        q_real = noise.compute_intensities(dt)[k]\n",
    "        loglik += torch.log(p_real / (p_real + q_real))\n",
    "        \n",
    "        # real non-event & noise event\n",
    "        for dtj, kj in noise.draw(dt): \n",
    "            #print(f\"dtj = {dtj}\")\n",
    "            #print(f\"kj = {kj}\")\n",
    "            p_noise = model.compute_intensities(dtj)[kj]\n",
    "            q_noise = noise.compute_intensities(dtj)[kj]\n",
    "            loglik -= torch.log(p_noise + q_noise)\n",
    "        \n",
    "        # update model and noise with real history\n",
    "        model.forward(k, dt)\n",
    "        noise.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2229.4138; time cost = 0.1280\n",
      "Iter-1: log-likelihood = -2222.2319; time cost = 0.1171\n",
      "Iter-2: log-likelihood = -2211.9751; time cost = 0.1228\n",
      "Iter-3: log-likelihood = -2199.3914; time cost = 0.1178\n",
      "Iter-4: log-likelihood = -2185.8748; time cost = 0.1093\n",
      "Iter-5: log-likelihood = -2170.5784; time cost = 0.1091\n",
      "Iter-6: log-likelihood = -2153.3032; time cost = 0.1190\n",
      "Iter-7: log-likelihood = -2134.7344; time cost = 0.1110\n",
      "Iter-8: log-likelihood = -2115.0427; time cost = 0.1154\n",
      "Iter-9: log-likelihood = -2094.1021; time cost = 0.1165\n",
      "Iter-10: log-likelihood = -2071.9785; time cost = 0.1173\n",
      "Iter-11: log-likelihood = -2049.8616; time cost = 0.1133\n",
      "Iter-12: log-likelihood = -2026.9714; time cost = 0.1117\n",
      "Iter-13: log-likelihood = -2003.6262; time cost = 0.1200\n",
      "Iter-14: log-likelihood = -1979.0774; time cost = 0.1128\n",
      "Iter-15: log-likelihood = -1954.2950; time cost = 0.1180\n",
      "Iter-16: log-likelihood = -1929.0826; time cost = 0.1103\n",
      "Iter-17: log-likelihood = -1902.9490; time cost = 0.1152\n",
      "Iter-18: log-likelihood = -1877.2090; time cost = 0.1146\n",
      "Iter-19: log-likelihood = -1850.7430; time cost = 0.1099\n",
      "Iter-20: log-likelihood = -1823.3899; time cost = 0.1158\n",
      "Iter-21: log-likelihood = -1794.3011; time cost = 0.1174\n",
      "Iter-22: log-likelihood = -1765.0516; time cost = 0.1143\n",
      "Iter-23: log-likelihood = -1734.4307; time cost = 0.1135\n",
      "Iter-24: log-likelihood = -1703.3838; time cost = 0.1124\n",
      "Iter-25: log-likelihood = -1671.0685; time cost = 0.1170\n",
      "Iter-26: log-likelihood = -1637.3136; time cost = 0.1140\n",
      "Iter-27: log-likelihood = -1603.0198; time cost = 0.1126\n",
      "Iter-28: log-likelihood = -1567.5897; time cost = 0.1108\n",
      "Iter-29: log-likelihood = -1531.9194; time cost = 0.1140\n",
      "Iter-30: log-likelihood = -1494.6644; time cost = 0.1131\n"
     ]
    }
   ],
   "source": [
    "model = NHP(kdim, hdim)\n",
    "noise = SimpleNoise(kdim, len(seq)*1.0/T)\n",
    "\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "MAX_ITER = 50\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = nce(seq, model, noise)\n",
    "    loss = -loglik \n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    loglik = mle(seq, model)\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next event time and type by sampling (approx. MBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time(model): \n",
    "    dts, ks = [], []\n",
    "    n = 10 \n",
    "    k, dt = thinning(model)\n",
    "    dts += [float(dt)]\n",
    "    dt_pred = np.mean(dts)\n",
    "    return dt_pred\n",
    "\n",
    "def predict_type(model, dt): \n",
    "    intens = model.compute_intensities(dt)\n",
    "    k_pred = torch.argmax(intens)\n",
    "    return k_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check time prediction accuracy\n",
      "RMSE using true model : 0.4117\n",
      "\n",
      "check type prediction error rate\n",
      "Error Rate using true model : 73.00%\n"
     ]
    }
   ],
   "source": [
    "se = 0.0\n",
    "nerr = 0\n",
    "\n",
    "nhp.start() # restart\n",
    "n = 100\n",
    "\n",
    "for i, s in enumerate(seq[:n]): \n",
    "    # predict\n",
    "    dt_pred = predict_time(nhp)\n",
    "    # time\n",
    "    dt = seq[i][0]\n",
    "    se += (dt_pred - dt) ** 2\n",
    "    # type \n",
    "    k_pred = predict_type(nhp, dt)\n",
    "    k = seq[i][1]\n",
    "    if k_pred != k: \n",
    "        nerr += 1\n",
    "\n",
    "print(f\"check time prediction accuracy\")\n",
    "#print(f\"RMSE using estimated intensity : {rmse_mle:.4f}\")\n",
    "print(f\"RMSE using true model : {np.sqrt(se/n):.4f}\")\n",
    "\n",
    "print(f\"\\ncheck type prediction error rate\")\n",
    "#print(f\"Error Rate using estimated intensities : {100.0*nerr_mle/len(seq):.2f}%\")\n",
    "print(f\"Error Rate using true model : {100.0*nerr/n:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tppcourse",
   "language": "python",
   "name": "tppcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
