{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Lecture-3 of Short Course of Temporal Point Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11100cb10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed \n",
    "np.random.seed(12345)\n",
    "torch.random.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Hawkes Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-time LSTM cell\n",
    "\n",
    "The LSTM cell $c(t)$ drifts from $c_{\\text{start}}$ towards $c_{\\text{target}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTLSTMCell(nn.Module): \n",
    "    \n",
    "    def __init__(self, hdim): \n",
    "        super(CTLSTMCell, self).__init__()\n",
    "        \"\"\"\n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.hdim = hdim \n",
    "        self.lin = nn.Linear(hdim*2, hdim*6, bias=True)\n",
    "        return \n",
    "    \n",
    "    def forward(self, x, h, c, ct): \n",
    "        \"\"\"\n",
    "        x : input embedding\n",
    "        h : hidden state right before time t \n",
    "        c : LSTM cell right before time t \n",
    "        ct : LSTM target cell given current history\n",
    "        \"\"\"\n",
    "        x = torch.cat((x, h), dim=0)\n",
    "        y = self.lin(x)\n",
    "        \n",
    "        gi, gf, z, git, gft, gd = y.chunk(6, 0)\n",
    "        \n",
    "        gi = torch.sigmoid(gi)\n",
    "        gf = torch.sigmoid(gf)\n",
    "        z = torch.tanh(z)\n",
    "        git = torch.sigmoid(git)\n",
    "        gft = torch.sigmoid(gft)\n",
    "        gd = F.softplus(gd)\n",
    "        \n",
    "        cs = gf * c + gi * z \n",
    "        ct = gft * ct + git * z\n",
    "        \n",
    "        return cs, ct, gd\n",
    "    \n",
    "    def decay(self, cs, ct, gd, dt): \n",
    "        \"\"\"\n",
    "        cs : LSTM start cell\n",
    "        ct : LSTM target cell \n",
    "        gd : decay gate\n",
    "        dt : elapsed time \n",
    "        \"\"\"\n",
    "        c = ct + (cs - ct) * torch.exp(-gd * dt)\n",
    "        h = torch.tanh(c)\n",
    "        \n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Hawkes process\n",
    "\n",
    "The intensity is defined as $\\lambda_k(t) = \\text{Softplus}(\\text{Linear}(h(t)))$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHP(nn.Module): \n",
    "    \n",
    "    def __init__(self, kdim, hdim): \n",
    "        super(NHP, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.eps = np.finfo(float).eps \n",
    "        self.max = np.finfo(float).max \n",
    "        self.kdim = kdim \n",
    "        self.hdim = hdim \n",
    "        self.BOS = kdim \n",
    "        \n",
    "        self.emb_in = nn.Embedding(kdim+1, hdim)\n",
    "        self.ctlstm = CTLSTMCell(hdim)\n",
    "        self.emb_out = nn.Linear(hdim, kdim)\n",
    "        \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.update(self.BOS, 0.0)\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        x = self.emb_in(torch.LongTensor([k]))[0]\n",
    "        self.cs, self.ct, self.gd = self.ctlstm(x, h, c, self.ct)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        return F.softplus(self.emb_out(h))\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        intensities = self.compute_intensities(dt)\n",
    "        return torch.sum(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sequence of events by thinning algorithm\n",
    "\n",
    "For the code to be easy to understand, I only have non-vectorized implementation. Please check the repos for my published papers for highly vectorized and optimized implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinning(model): \n",
    "    dt = 0.0\n",
    "    bound = 100.0 \n",
    "    # manualy chosen for simplicity\n",
    "    # in principle, it can be found using the method in Appendix B.3 of Mei & Eisner 2017\n",
    "    while True: \n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        dt += -np.log(1-u) / bound\n",
    "        intens = model.compute_intensities(dt)\n",
    "        total_inten = torch.sum(intens)\n",
    "        accept_prob = total_inten / bound\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        if u <= accept_prob: \n",
    "            break \n",
    "    \n",
    "    k = torch.multinomial(intens, 1)\n",
    "    \n",
    "    return k, dt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw data from a low-entropy distribution: (1) draw $dt$ from a univariate NHP; (2) draw $k$ from a n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over time interval [0, 100.0]\n",
      "# of events : 78\n"
     ]
    }
   ],
   "source": [
    "kdim = 32 \n",
    "hdim = 8\n",
    "nhp = NHP(1, hdim)\n",
    "# init by BOS \n",
    "nhp.start()\n",
    "\n",
    "T = 100.0\n",
    "t = 0\n",
    "seq = []\n",
    "CONTEXT = 0\n",
    "\n",
    "while True:\n",
    "    # draw dt using thinning algorithm\n",
    "    \n",
    "    _, dt = thinning(nhp)\n",
    "    t += dt\n",
    "    if t <= T: \n",
    "        k = (CONTEXT + 1) % kdim\n",
    "        seq += [(dt, k)] # track dt, not t, easy to use\n",
    "        # update model \n",
    "        nhp.forward(0, dt)\n",
    "        # update CONTEXT \n",
    "        CONTEXT = k\n",
    "    else: \n",
    "        break\n",
    "\n",
    "print(f\"over time interval [0, {T}]\")\n",
    "print(f\"# of events : {len(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(model, seq): \n",
    "    \"\"\"\n",
    "    compute log-likelihood of seq under model\n",
    "    \"\"\"\n",
    "    J = 10\n",
    "    model.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    for event in seq: \n",
    "        dt, k = event\n",
    "        # log intensity \n",
    "        loglik += torch.log(model.compute_intensities(dt)[k])\n",
    "        # integral\n",
    "        integral = 0.0 \n",
    "        for j in range(J): \n",
    "            # draw uniform-distributed time points\n",
    "            dtj = np.random.uniform(0.0, dt)\n",
    "            integral += model.compute_total_intensity(dtj)\n",
    "        integral /= J \n",
    "        integral *= dt \n",
    "        loglik -= integral\n",
    "        # update model\n",
    "        model.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2193.8508; time cost = 0.2922\n",
      "Iter-1: log-likelihood = -2081.7927; time cost = 0.2899\n",
      "Iter-2: log-likelihood = -1896.3030; time cost = 0.2956\n",
      "Iter-3: log-likelihood = -1647.4878; time cost = 0.2997\n",
      "Iter-4: log-likelihood = -1329.7615; time cost = 0.2988\n",
      "Iter-5: log-likelihood = -987.4490; time cost = 0.2885\n",
      "Iter-6: log-likelihood = -700.0302; time cost = 0.2880\n",
      "Iter-7: log-likelihood = -507.2344; time cost = 0.3033\n",
      "Iter-8: log-likelihood = -408.5676; time cost = 0.2985\n",
      "Iter-9: log-likelihood = -373.2642; time cost = 0.2969\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "model = NHP(kdim, hdim)\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "MAX_ITER = 10\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = mle(model, seq)\n",
    "    loss = -loglik \n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next event time and type by sampling (approx. MBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time(model): \n",
    "    dts, ks = [], []\n",
    "    n = 10 \n",
    "    k, dt = thinning(model)\n",
    "    dts += [float(dt)]\n",
    "    dt_pred = np.mean(dts)\n",
    "    return dt_pred\n",
    "\n",
    "def predict_type(model, dt): \n",
    "    intens = model.compute_intensities(dt)\n",
    "    k_pred = torch.argmax(intens)\n",
    "    return k_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check time prediction accuracy\n",
      "RMSE using true model : 0.4117\n",
      "\n",
      "check type prediction error rate\n",
      "Error Rate using true model : 73.00%\n"
     ]
    }
   ],
   "source": [
    "se = 0.0\n",
    "nerr = 0\n",
    "\n",
    "nhp.start() # restart\n",
    "n = 100\n",
    "\n",
    "for i, s in enumerate(seq[:n]): \n",
    "    # predict\n",
    "    dt_pred = predict_time(nhp)\n",
    "    # time\n",
    "    dt = seq[i][0]\n",
    "    se += (dt_pred - dt) ** 2\n",
    "    # type \n",
    "    k_pred = predict_type(nhp, dt)\n",
    "    k = seq[i][1]\n",
    "    if k_pred != k: \n",
    "        nerr += 1\n",
    "\n",
    "print(f\"check time prediction accuracy\")\n",
    "#print(f\"RMSE using estimated intensity : {rmse_mle:.4f}\")\n",
    "print(f\"RMSE using true model : {np.sqrt(se/n):.4f}\")\n",
    "\n",
    "print(f\"\\ncheck type prediction error rate\")\n",
    "#print(f\"Error Rate using estimated intensities : {100.0*nerr_mle/len(seq):.2f}%\")\n",
    "print(f\"Error Rate using true model : {100.0*nerr/n:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tppcourse",
   "language": "python",
   "name": "tppcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
