{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Lecture-3 of Short Course of Temporal Point Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11100cb10>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed \n",
    "np.random.seed(12345)\n",
    "torch.random.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Hawkes Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-time LSTM cell\n",
    "\n",
    "The LSTM cell $c(t)$ drifts from $c_{\\text{start}}$ towards $c_{\\text{target}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTLSTMCell(nn.Module): \n",
    "    \n",
    "    def __init__(self, hdim): \n",
    "        super(CTLSTMCell, self).__init__()\n",
    "        \"\"\"\n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.hdim = hdim \n",
    "        self.lin = nn.Linear(hdim*2, hdim*6, bias=True)\n",
    "        return \n",
    "    \n",
    "    def forward(self, x, h, c, ct): \n",
    "        \"\"\"\n",
    "        x : input embedding\n",
    "        h : hidden state right before time t \n",
    "        c : LSTM cell right before time t \n",
    "        ct : LSTM target cell given current history\n",
    "        \"\"\"\n",
    "        x = torch.cat((x, h), dim=0)\n",
    "        y = self.lin(x)\n",
    "        \n",
    "        gi, gf, z, git, gft, gd = y.chunk(6, 0)\n",
    "        \n",
    "        gi = torch.sigmoid(gi)\n",
    "        gf = torch.sigmoid(gf)\n",
    "        z = torch.tanh(z)\n",
    "        git = torch.sigmoid(git)\n",
    "        gft = torch.sigmoid(gft)\n",
    "        gd = F.softplus(gd)\n",
    "        \n",
    "        cs = gf * c + gi * z \n",
    "        ct = gft * ct + git * z\n",
    "        \n",
    "        return cs, ct, gd\n",
    "    \n",
    "    def decay(self, cs, ct, gd, dt): \n",
    "        \"\"\"\n",
    "        cs : LSTM start cell\n",
    "        ct : LSTM target cell \n",
    "        gd : decay gate\n",
    "        dt : elapsed time \n",
    "        \"\"\"\n",
    "        c = ct + (cs - ct) * torch.exp(-gd * dt)\n",
    "        h = torch.tanh(c)\n",
    "        \n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Hawkes process\n",
    "\n",
    "The intensity is defined as $\\lambda_k(t) = \\text{Softplus}(\\text{Linear}(h(t)))$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHP(nn.Module): \n",
    "    \n",
    "    def __init__(self, kdim, hdim): \n",
    "        super(NHP, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.eps = np.finfo(float).eps \n",
    "        self.max = np.finfo(float).max \n",
    "        self.kdim = kdim \n",
    "        self.hdim = hdim \n",
    "        self.BOS = kdim \n",
    "        \n",
    "        self.emb_in = nn.Embedding(kdim+1, hdim)\n",
    "        self.ctlstm = CTLSTMCell(hdim)\n",
    "        self.emb_out = nn.Linear(hdim, kdim)\n",
    "        \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.update(self.BOS, 0.0)\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        x = self.emb_in(torch.LongTensor([k]))[0]\n",
    "        self.cs, self.ct, self.gd = self.ctlstm(x, h, c, self.ct)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        return F.softplus(self.emb_out(h))\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        intensities = self.compute_intensities(dt)\n",
    "        return torch.sum(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sequence of events by thinning algorithm\n",
    "\n",
    "For the code to be easy to understand, I only have non-vectorized implementation. Please check the repos for my published papers for highly vectorized and optimized implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinning(model): \n",
    "    dt = 0.0\n",
    "    bound = 100.0 \n",
    "    # manualy chosen for simplicity\n",
    "    # in principle, it can be found using the method in Appendix B.3 of Mei & Eisner 2017\n",
    "    while True: \n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        dt += -np.log(1-u) / bound\n",
    "        intens = model.compute_intensities(dt)\n",
    "        total_inten = torch.sum(intens)\n",
    "        accept_prob = total_inten / bound\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        if u <= accept_prob: \n",
    "            break \n",
    "    \n",
    "    k = torch.multinomial(intens, 1)\n",
    "    \n",
    "    return k, dt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw data from a low-entropy distribution: (1) draw $dt$ from a univariate NHP; (2) draw $k$ from a n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over time interval [0, 100.0]\n",
      "# of events : 75\n"
     ]
    }
   ],
   "source": [
    "kdim = 32 \n",
    "hdim = 8\n",
    "nhp = NHP(1, hdim)\n",
    "# init by BOS \n",
    "nhp.start()\n",
    "\n",
    "T = 100.0\n",
    "t = 0\n",
    "seq = []\n",
    "CONTEXT = 0\n",
    "\n",
    "while True:\n",
    "    # draw dt using thinning algorithm\n",
    "    \n",
    "    _, dt = thinning(nhp)\n",
    "    t += dt\n",
    "    if t <= T: \n",
    "        k = (CONTEXT + 1) % kdim # increase event type ID by +1 mod K\n",
    "        seq += [(dt, k)] # track dt, not t, easy to use\n",
    "        # update model \n",
    "        nhp.forward(0, dt)\n",
    "        # update CONTEXT \n",
    "        CONTEXT = k\n",
    "    else: \n",
    "        break\n",
    "\n",
    "print(f\"over time interval [0, {T}]\")\n",
    "print(f\"# of events : {len(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(data, model): \n",
    "    \"\"\"\n",
    "    compute log-likelihood of seq under model\n",
    "    \"\"\"\n",
    "    J = 10\n",
    "    model.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    sum log intensity - integral total intensity\n",
    "    \"\"\"\n",
    "    \n",
    "    for event in seq: \n",
    "        dt, k = event\n",
    "        # log intensity \n",
    "        loglik += torch.log(model.compute_intensities(dt)[k])\n",
    "        # integral\n",
    "        integral = 0.0 \n",
    "        for j in range(J): \n",
    "            # draw uniform-distributed time points\n",
    "            dtj = np.random.uniform(0.0, dt)\n",
    "            integral += model.compute_total_intensity(dtj)\n",
    "        integral /= J \n",
    "        integral *= dt \n",
    "        loglik -= integral\n",
    "        # update model\n",
    "        model.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2256.1953; time cost = 0.2943\n",
      "Iter-1: log-likelihood = -2165.5391; time cost = 0.2845\n",
      "Iter-2: log-likelihood = -1996.2584; time cost = 0.2839\n",
      "Iter-3: log-likelihood = -1750.4049; time cost = 0.2855\n",
      "Iter-4: log-likelihood = -1433.0594; time cost = 0.2840\n",
      "Iter-5: log-likelihood = -1089.5035; time cost = 0.2864\n",
      "Iter-6: log-likelihood = -784.1178; time cost = 0.2885\n",
      "Iter-7: log-likelihood = -563.9142; time cost = 0.2905\n",
      "Iter-8: log-likelihood = -436.5165; time cost = 0.2886\n",
      "Iter-9: log-likelihood = -377.5184; time cost = 0.2886\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "model = NHP(kdim, hdim) # model to train\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # SGD \n",
    "\n",
    "MAX_ITER = 10\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = mle(seq, model) # compute log-likelihood\n",
    "    loss = -loglik \n",
    "    loss.backward() # compute gradient\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next event time and type by sampling (approx. MBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time(model): \n",
    "    dts, ks = [], []\n",
    "    n = 10 \n",
    "    k, dt = thinning(model)\n",
    "    dts += [float(dt)]\n",
    "    dt_pred = np.mean(dts)\n",
    "    return dt_pred\n",
    "\n",
    "def predict_type(model, dt): \n",
    "    intens = model.compute_intensities(dt)\n",
    "    k_pred = torch.argmax(intens)\n",
    "    return k_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check time prediction accuracy\n",
      "RMSE using trained model : 1.2962\n",
      "\n",
      "check type prediction error rate\n",
      "Error Rate using true model : 72.00%\n",
      "Error Rate of random guess : 96.88%\n"
     ]
    }
   ],
   "source": [
    "se = 0.0\n",
    "nerr = 0\n",
    "\n",
    "model.start() # restart\n",
    "n = 100\n",
    "\n",
    "for i, s in enumerate(seq[:n]): \n",
    "    # predict\n",
    "    dt_pred = predict_time(model)\n",
    "    # time\n",
    "    dt = seq[i][0]\n",
    "    se += (dt_pred - dt) ** 2\n",
    "    # type \n",
    "    k_pred = predict_type(model, dt)\n",
    "    k = seq[i][1]\n",
    "    if k_pred != k: \n",
    "        nerr += 1\n",
    "\n",
    "print(f\"check time prediction accuracy\")\n",
    "print(f\"RMSE using trained model : {np.sqrt(se/n):.4f}\")\n",
    "\n",
    "print(f\"\\ncheck type prediction error rate\")\n",
    "print(f\"Error Rate using true model : {100.0*nerr/n:.2f}%\")\n",
    "print(f\"Error Rate of random guess : {100.0*(1-1/kdim):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNoise(nn.Module): \n",
    "    \"\"\"\n",
    "    a simple noise distribution -- Poisson process\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kdim, total_intensity): \n",
    "        super(SimpleNoise, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        \"\"\"\n",
    "        self.total_intensity = total_intensity\n",
    "        self.inten = total_intensity / kdim\n",
    "        self.intens = torch.zeros([kdim], dtype = torch.float32).fill_(self.inten)\n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        # do nothing\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        # does nothing\n",
    "        # simple distribution, no dependence on history\n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        return self.intens\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        return self.total_intensity\n",
    "    \n",
    "    def draw(self, dt): \n",
    "        noise_events = [] # a collection of noise events over given interval\n",
    "        dtj = 0.0\n",
    "        while True: \n",
    "            # draw noise time (inversion sampling)\n",
    "            u = np.random.uniform(0.0, 1.0)\n",
    "            dtj += -np.log(1-u) / self.total_intensity\n",
    "            \n",
    "            if dtj <= dt: \n",
    "                # draw noise type\n",
    "                kj = torch.multinomial(self.intens, 1)[0]\n",
    "                noise_events += [(dtj, kj)]\n",
    "            else: \n",
    "                break \n",
    "        \n",
    "        return noise_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce(data, model, noise): \n",
    "    \"\"\"\n",
    "    compute log-probability of correct discrimination under model and noise\n",
    "    \"\"\"\n",
    "    model.start()\n",
    "    noise.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    for dt, k in seq: \n",
    "        \n",
    "        # real event & noise non-event\n",
    "        p_real = model.compute_intensities(dt)[k]\n",
    "        q_real = noise.compute_intensities(dt)[k]\n",
    "        loglik += torch.log(p_real / (p_real + q_real))\n",
    "        \n",
    "        # real non-event & noise event\n",
    "        for dtj, kj in noise.draw(dt): \n",
    "            p_noise = model.compute_intensities(dtj)[kj]\n",
    "            q_noise = noise.compute_intensities(dtj)[kj]\n",
    "            loglik += torch.log(q_noise / (p_noise + q_noise))\n",
    "        \n",
    "        # update model and noise with real event\n",
    "        # both model and noise conditioned on real history\n",
    "        model.forward(k, dt)\n",
    "        noise.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2197.9480; time cost = 0.1297\n",
      "Iter-1: log-likelihood = -2187.5435; time cost = 0.1210\n",
      "Iter-2: log-likelihood = -2172.8279; time cost = 0.1193\n",
      "Iter-3: log-likelihood = -2155.7124; time cost = 0.1128\n",
      "Iter-4: log-likelihood = -2135.2249; time cost = 0.1130\n",
      "Iter-5: log-likelihood = -2111.0044; time cost = 0.1148\n",
      "Iter-6: log-likelihood = -2082.8306; time cost = 0.1194\n",
      "Iter-7: log-likelihood = -2051.7053; time cost = 0.1137\n",
      "Iter-8: log-likelihood = -2017.0199; time cost = 0.1158\n",
      "Iter-9: log-likelihood = -1980.0520; time cost = 0.1146\n",
      "Iter-10: log-likelihood = -1940.0547; time cost = 0.1133\n",
      "Iter-11: log-likelihood = -1897.7214; time cost = 0.1201\n",
      "Iter-12: log-likelihood = -1852.1503; time cost = 0.1157\n",
      "Iter-13: log-likelihood = -1807.4056; time cost = 0.1086\n",
      "Iter-14: log-likelihood = -1760.8628; time cost = 0.1124\n",
      "Iter-15: log-likelihood = -1712.9430; time cost = 0.1108\n",
      "Iter-16: log-likelihood = -1663.4989; time cost = 0.1224\n",
      "Iter-17: log-likelihood = -1610.7399; time cost = 0.1256\n",
      "Iter-18: log-likelihood = -1557.7058; time cost = 0.1232\n",
      "Iter-19: log-likelihood = -1501.2209; time cost = 0.1343\n",
      "Iter-20: log-likelihood = -1442.9784; time cost = 0.1565\n",
      "Iter-21: log-likelihood = -1384.2220; time cost = 0.1672\n",
      "Iter-22: log-likelihood = -1324.4274; time cost = 0.1203\n",
      "Iter-23: log-likelihood = -1263.2023; time cost = 0.1235\n",
      "Iter-24: log-likelihood = -1200.8335; time cost = 0.1216\n",
      "Iter-25: log-likelihood = -1138.4518; time cost = 0.1269\n",
      "Iter-26: log-likelihood = -1076.3527; time cost = 0.1181\n",
      "Iter-27: log-likelihood = -1015.3956; time cost = 0.1168\n",
      "Iter-28: log-likelihood = -954.8250; time cost = 0.1236\n",
      "Iter-29: log-likelihood = -897.6727; time cost = 0.1203\n",
      "Iter-30: log-likelihood = -845.1367; time cost = 0.1115\n",
      "Iter-31: log-likelihood = -794.6874; time cost = 0.1211\n",
      "Iter-32: log-likelihood = -745.7866; time cost = 0.1247\n",
      "Iter-33: log-likelihood = -701.3091; time cost = 0.1178\n",
      "Iter-34: log-likelihood = -658.5037; time cost = 0.1210\n",
      "Iter-35: log-likelihood = -619.3153; time cost = 0.1273\n",
      "Iter-36: log-likelihood = -584.7917; time cost = 0.1086\n",
      "Iter-37: log-likelihood = -552.9203; time cost = 0.1171\n",
      "Iter-38: log-likelihood = -523.8386; time cost = 0.1234\n",
      "Iter-39: log-likelihood = -495.8834; time cost = 0.1324\n",
      "Iter-40: log-likelihood = -471.4336; time cost = 0.1195\n",
      "Iter-41: log-likelihood = -450.2753; time cost = 0.1216\n",
      "Iter-42: log-likelihood = -432.7843; time cost = 0.1411\n",
      "Iter-43: log-likelihood = -417.6917; time cost = 0.1219\n",
      "Iter-44: log-likelihood = -404.8846; time cost = 0.1210\n",
      "Iter-45: log-likelihood = -394.8807; time cost = 0.1137\n",
      "Iter-46: log-likelihood = -386.9372; time cost = 0.1339\n",
      "Iter-47: log-likelihood = -380.4537; time cost = 0.1220\n",
      "Iter-48: log-likelihood = -375.3394; time cost = 0.1149\n",
      "Iter-49: log-likelihood = -371.4430; time cost = 0.1099\n"
     ]
    }
   ],
   "source": [
    "model = NHP(kdim, hdim) # model to train\n",
    "noise = SimpleNoise(kdim, len(seq)*1.0/T) # noise distribution q \n",
    "\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "MAX_ITER = 50\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = nce(seq, model, noise)\n",
    "    loss = -loglik \n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    loglik = mle(seq, model)\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tppcourse",
   "language": "python",
   "name": "tppcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
