{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Lecture-3 of Short Course of Temporal Point Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11100cb10>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed \n",
    "np.random.seed(12345)\n",
    "torch.random.manual_seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Hawkes Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-time LSTM cell\n",
    "\n",
    "The LSTM cell $c(t)$ drifts from $c_{\\text{start}}$ towards $c_{\\text{target}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTLSTMCell(nn.Module): \n",
    "    \n",
    "    def __init__(self, hdim): \n",
    "        super(CTLSTMCell, self).__init__()\n",
    "        \"\"\"\n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.hdim = hdim \n",
    "        self.lin = nn.Linear(hdim*2, hdim*6, bias=True)\n",
    "        return \n",
    "    \n",
    "    def forward(self, x, h, c, ct): \n",
    "        \"\"\"\n",
    "        x : input embedding\n",
    "        h : hidden state right before time t \n",
    "        c : LSTM cell right before time t \n",
    "        ct : LSTM target cell given current history\n",
    "        \"\"\"\n",
    "        x = torch.cat((x, h), dim=0)\n",
    "        y = self.lin(x)\n",
    "        \n",
    "        gi, gf, z, git, gft, gd = y.chunk(6, 0)\n",
    "        \n",
    "        gi = torch.sigmoid(gi)\n",
    "        gf = torch.sigmoid(gf)\n",
    "        z = torch.tanh(z)\n",
    "        git = torch.sigmoid(git)\n",
    "        gft = torch.sigmoid(gft)\n",
    "        gd = F.softplus(gd)\n",
    "        \n",
    "        cs = gf * c + gi * z \n",
    "        ct = gft * ct + git * z\n",
    "        \n",
    "        return cs, ct, gd\n",
    "    \n",
    "    def decay(self, cs, ct, gd, dt): \n",
    "        \"\"\"\n",
    "        cs : LSTM start cell\n",
    "        ct : LSTM target cell \n",
    "        gd : decay gate\n",
    "        dt : elapsed time \n",
    "        \"\"\"\n",
    "        c = ct + (cs - ct) * torch.exp(-gd * dt)\n",
    "        h = torch.tanh(c)\n",
    "        \n",
    "        return c, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Hawkes process\n",
    "\n",
    "The intensity is defined as $\\lambda_k(t) = \\text{Softplus}(\\text{Linear}(h(t)))$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NHP(nn.Module): \n",
    "    \n",
    "    def __init__(self, kdim, hdim): \n",
    "        super(NHP, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        hdim : # of hidden neurons\n",
    "        \"\"\"\n",
    "        self.eps = np.finfo(float).eps \n",
    "        self.max = np.finfo(float).max \n",
    "        self.kdim = kdim \n",
    "        self.hdim = hdim \n",
    "        self.BOS = kdim \n",
    "        \n",
    "        self.emb_in = nn.Embedding(kdim+1, hdim)\n",
    "        self.ctlstm = CTLSTMCell(hdim)\n",
    "        self.emb_out = nn.Linear(hdim, kdim)\n",
    "        \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        self.cs = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.ct = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.gd = torch.zeros(size=[hdim], dtype=torch.float32)\n",
    "        self.update(self.BOS, 0.0)\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        x = self.emb_in(torch.LongTensor([k]))[0]\n",
    "        self.cs, self.ct, self.gd = self.ctlstm(x, h, c, self.ct)\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        c, h = self.ctlstm.decay(self.cs, self.ct, self.gd, dt)\n",
    "        return F.softplus(self.emb_out(h))\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        intensities = self.compute_intensities(dt)\n",
    "        return torch.sum(intensities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sequence of events by thinning algorithm\n",
    "\n",
    "For the code to be easy to understand, I only have non-vectorized implementation. Please check the repos for my published papers for highly vectorized and optimized implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thinning(model): \n",
    "    dt = 0.0\n",
    "    bound = 100.0 \n",
    "    # manualy chosen for simplicity\n",
    "    # in principle, it can be found using the method in Appendix B.3 of Mei & Eisner 2017\n",
    "    while True: \n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        dt += -np.log(1-u) / bound\n",
    "        intens = model.compute_intensities(dt)\n",
    "        total_inten = torch.sum(intens)\n",
    "        accept_prob = total_inten / bound\n",
    "        u = np.random.uniform(0.0, 1.0)\n",
    "        if u <= accept_prob: \n",
    "            break \n",
    "    \n",
    "    k = torch.multinomial(intens, 1)\n",
    "    \n",
    "    return k, dt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw data from a low-entropy distribution: (1) draw $dt$ from a univariate NHP; (2) draw $k$ from a n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over time interval [0, 100.0]\n",
      "# of events : 88\n"
     ]
    }
   ],
   "source": [
    "kdim = 32 \n",
    "hdim = 8\n",
    "nhp = NHP(1, hdim)\n",
    "# init by BOS \n",
    "nhp.start()\n",
    "\n",
    "T = 100.0\n",
    "t = 0\n",
    "seq = []\n",
    "CONTEXT = 0\n",
    "\n",
    "while True:\n",
    "    # draw dt using thinning algorithm\n",
    "    \n",
    "    _, dt = thinning(nhp)\n",
    "    t += dt\n",
    "    if t <= T: \n",
    "        k = (CONTEXT + 1) % kdim # increase event type ID by +1 mod K\n",
    "        seq += [(dt, k)] # track dt, not t, easy to use\n",
    "        # update model \n",
    "        nhp.forward(0, dt)\n",
    "        # update CONTEXT \n",
    "        CONTEXT = k\n",
    "    else: \n",
    "        break\n",
    "\n",
    "print(f\"over time interval [0, {T}]\")\n",
    "print(f\"# of events : {len(seq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle(data, model): \n",
    "    \"\"\"\n",
    "    compute log-likelihood of seq under model\n",
    "    \"\"\"\n",
    "    J = 10\n",
    "    model.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    \"\"\"\n",
    "    sum log intensity - integral total intensity\n",
    "    \"\"\"\n",
    "    \n",
    "    for event in seq: \n",
    "        dt, k = event\n",
    "        # log intensity \n",
    "        loglik += torch.log(model.compute_intensities(dt)[k])\n",
    "        # integral\n",
    "        integral = 0.0 \n",
    "        for j in range(J): \n",
    "            # draw uniform-distributed time points\n",
    "            dtj = np.random.uniform(0.0, dt)\n",
    "            integral += model.compute_total_intensity(dtj)\n",
    "        integral /= J \n",
    "        integral *= dt \n",
    "        loglik -= integral\n",
    "        # update model\n",
    "        model.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2242.1567; time cost = 0.6457\n",
      "Iter-1: log-likelihood = -2153.4778; time cost = 0.6506\n",
      "Iter-2: log-likelihood = -1988.6143; time cost = 0.7776\n",
      "Iter-3: log-likelihood = -1753.1870; time cost = 0.8019\n",
      "Iter-4: log-likelihood = -1449.4432; time cost = 0.8723\n",
      "Iter-5: log-likelihood = -1103.1483; time cost = 0.8863\n",
      "Iter-6: log-likelihood = -784.9104; time cost = 0.8146\n",
      "Iter-7: log-likelihood = -568.1177; time cost = 0.8153\n",
      "Iter-8: log-likelihood = -455.4929; time cost = 0.8165\n",
      "Iter-9: log-likelihood = -412.7558; time cost = 0.8815\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "\n",
    "model = NHP(kdim, hdim) # model to train\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # SGD \n",
    "\n",
    "MAX_ITER = 10\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = mle(seq, model) # compute log-likelihood\n",
    "    loss = -loglik \n",
    "    loss.backward() # compute gradient\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict next event time and type by sampling (approx. MBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_time(model): \n",
    "    dts, ks = [], []\n",
    "    n = 10 \n",
    "    k, dt = thinning(model)\n",
    "    dts += [float(dt)]\n",
    "    dt_pred = np.mean(dts)\n",
    "    return dt_pred\n",
    "\n",
    "def predict_type(model, dt): \n",
    "    intens = model.compute_intensities(dt)\n",
    "    k_pred = torch.argmax(intens)\n",
    "    return k_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check time prediction accuracy\n",
      "RMSE using trained model : 1.4219\n",
      "\n",
      "check type prediction error rate\n",
      "Error Rate using true model : 85.00%\n",
      "Error Rate of random guess : 96.88%\n"
     ]
    }
   ],
   "source": [
    "se = 0.0\n",
    "nerr = 0\n",
    "\n",
    "model.start() # restart\n",
    "n = 100\n",
    "\n",
    "for i, s in enumerate(seq[:n]): \n",
    "    # predict\n",
    "    dt_pred = predict_time(model)\n",
    "    # time\n",
    "    dt = seq[i][0]\n",
    "    se += (dt_pred - dt) ** 2\n",
    "    # type \n",
    "    k_pred = predict_type(model, dt)\n",
    "    k = seq[i][1]\n",
    "    if k_pred != k: \n",
    "        nerr += 1\n",
    "\n",
    "print(f\"check time prediction accuracy\")\n",
    "print(f\"RMSE using trained model : {np.sqrt(se/n):.4f}\")\n",
    "\n",
    "print(f\"\\ncheck type prediction error rate\")\n",
    "print(f\"Error Rate using true model : {100.0*nerr/n:.2f}%\")\n",
    "print(f\"Error Rate of random guess : {100.0*(1-1/kdim):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NHP by NCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNoise(nn.Module): \n",
    "    \"\"\"\n",
    "    a simple noise distribution -- Poisson process\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, kdim, total_intensity): \n",
    "        super(SimpleNoise, self).__init__()\n",
    "        \"\"\"\n",
    "        kdim : # of event types \n",
    "        \"\"\"\n",
    "        self.total_intensity = total_intensity\n",
    "        self.inten = total_intensity / kdim\n",
    "        self.intens = torch.zeros([kdim], dtype = torch.float32).fill_(self.inten)\n",
    "        return \n",
    "    \n",
    "    def start(self): \n",
    "        # do nothing\n",
    "        return \n",
    "    \n",
    "    def update(self, k, dt): \n",
    "        \"\"\"\n",
    "        k : event type \n",
    "        dt : elapsed time since last event\n",
    "        \"\"\"\n",
    "        # does nothing\n",
    "        # simple distribution, no dependence on history\n",
    "        return \n",
    "    \n",
    "    def forward(self, k, dt): \n",
    "        self.update(k, dt)\n",
    "        return \n",
    "    \n",
    "    def compute_intensities(self, dt): \n",
    "        return self.intens\n",
    "    \n",
    "    def compute_total_intensity(self, dt): \n",
    "        return self.total_intensity\n",
    "    \n",
    "    def draw(self, dt): \n",
    "        noise_events = [] # a collection of noise events over given interval\n",
    "        dtj = 0.0\n",
    "        while True: \n",
    "            # draw noise time (inversion sampling)\n",
    "            u = np.random.uniform(0.0, 1.0)\n",
    "            dtj += -np.log(1-u) / self.total_intensity\n",
    "            \n",
    "            if dtj <= dt: \n",
    "                # draw noise type\n",
    "                kj = torch.multinomial(self.intens, 1)[0]\n",
    "                noise_events += [(dtj, kj)]\n",
    "            else: \n",
    "                break \n",
    "        \n",
    "        return noise_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce(data, model, noise): \n",
    "    \"\"\"\n",
    "    compute log-probability of correct discrimination under model and noise\n",
    "    \"\"\"\n",
    "    model.start()\n",
    "    noise.start()\n",
    "    loglik = 0.0\n",
    "    \n",
    "    for dt, k in seq: \n",
    "        \n",
    "        # real event & noise non-event\n",
    "        p_real = model.compute_intensities(dt)[k]\n",
    "        q_real = noise.compute_intensities(dt)[k]\n",
    "        loglik += torch.log(p_real / (p_real + q_real))\n",
    "        \n",
    "        # real non-event & noise event\n",
    "        for dtj, kj in noise.draw(dt): \n",
    "            p_noise = model.compute_intensities(dtj)[kj]\n",
    "            q_noise = noise.compute_intensities(dtj)[kj]\n",
    "            loglik += torch.log(q_noise / (p_noise + q_noise))\n",
    "        \n",
    "        # update model and noise with real event\n",
    "        # both model and noise conditioned on real history\n",
    "        model.forward(k, dt)\n",
    "        noise.forward(k, dt)\n",
    "    \n",
    "    return loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter-0: log-likelihood = -2330.4666; time cost = 0.2368\n",
      "Iter-1: log-likelihood = -2320.1770; time cost = 0.2308\n",
      "Iter-2: log-likelihood = -2306.1011; time cost = 0.2290\n",
      "Iter-3: log-likelihood = -2288.8208; time cost = 0.2474\n",
      "Iter-4: log-likelihood = -2268.6750; time cost = 0.2579\n",
      "Iter-5: log-likelihood = -2246.9258; time cost = 0.2637\n",
      "Iter-6: log-likelihood = -2222.8083; time cost = 0.2683\n",
      "Iter-7: log-likelihood = -2196.7983; time cost = 0.2964\n",
      "Iter-8: log-likelihood = -2167.4907; time cost = 0.2952\n",
      "Iter-9: log-likelihood = -2137.0186; time cost = 0.2855\n",
      "Iter-10: log-likelihood = -2104.3833; time cost = 0.3175\n",
      "Iter-11: log-likelihood = -2068.4480; time cost = 0.3026\n",
      "Iter-12: log-likelihood = -2030.2980; time cost = 0.3029\n",
      "Iter-13: log-likelihood = -1990.9193; time cost = 0.3390\n",
      "Iter-14: log-likelihood = -1950.4828; time cost = 0.3859\n",
      "Iter-15: log-likelihood = -1908.9060; time cost = 0.3653\n",
      "Iter-16: log-likelihood = -1866.5612; time cost = 0.3577\n",
      "Iter-17: log-likelihood = -1821.6584; time cost = 0.3758\n",
      "Iter-18: log-likelihood = -1774.8082; time cost = 0.3587\n",
      "Iter-19: log-likelihood = -1725.2346; time cost = 0.4306\n",
      "Iter-20: log-likelihood = -1674.2202; time cost = 0.3416\n",
      "Iter-21: log-likelihood = -1621.4664; time cost = 0.3291\n",
      "Iter-22: log-likelihood = -1568.5070; time cost = 0.3088\n",
      "Iter-23: log-likelihood = -1514.1053; time cost = 0.3275\n",
      "Iter-24: log-likelihood = -1458.3494; time cost = 0.5277\n",
      "Iter-25: log-likelihood = -1400.9655; time cost = 0.3674\n",
      "Iter-26: log-likelihood = -1341.1646; time cost = 0.3794\n",
      "Iter-27: log-likelihood = -1280.8866; time cost = 0.3632\n",
      "Iter-28: log-likelihood = -1220.0604; time cost = 0.3818\n",
      "Iter-29: log-likelihood = -1159.8989; time cost = 0.3628\n",
      "Iter-30: log-likelihood = -1099.1298; time cost = 0.4075\n",
      "Iter-31: log-likelihood = -1038.8691; time cost = 0.3663\n",
      "Iter-32: log-likelihood = -981.9775; time cost = 0.3627\n",
      "Iter-33: log-likelihood = -928.0787; time cost = 0.3300\n",
      "Iter-34: log-likelihood = -875.3284; time cost = 0.3440\n",
      "Iter-35: log-likelihood = -825.5681; time cost = 0.4071\n",
      "Iter-36: log-likelihood = -778.8530; time cost = 0.3502\n",
      "Iter-37: log-likelihood = -734.0843; time cost = 0.3840\n",
      "Iter-38: log-likelihood = -691.7080; time cost = 0.4392\n",
      "Iter-39: log-likelihood = -653.2717; time cost = 0.3790\n",
      "Iter-40: log-likelihood = -619.3278; time cost = 0.3851\n",
      "Iter-41: log-likelihood = -588.3456; time cost = 0.3327\n",
      "Iter-42: log-likelihood = -559.6933; time cost = 0.4029\n",
      "Iter-43: log-likelihood = -533.8821; time cost = 0.4137\n",
      "Iter-44: log-likelihood = -511.4175; time cost = 0.4328\n",
      "Iter-45: log-likelihood = -491.8787; time cost = 0.3730\n",
      "Iter-46: log-likelihood = -475.8307; time cost = 0.3675\n",
      "Iter-47: log-likelihood = -461.7756; time cost = 0.3412\n",
      "Iter-48: log-likelihood = -450.3026; time cost = 0.3388\n",
      "Iter-49: log-likelihood = -441.0294; time cost = 0.3370\n"
     ]
    }
   ],
   "source": [
    "model = NHP(kdim, hdim) # model to train\n",
    "noise = SimpleNoise(kdim, len(seq)*1.0/T) # noise distribution q \n",
    "\n",
    "sgd = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "MAX_ITER = 50\n",
    "\n",
    "for i in range(MAX_ITER): \n",
    "    \n",
    "    tic = time.time()\n",
    "    sgd.zero_grad()\n",
    "    loglik = nce(seq, model, noise)\n",
    "    loss = -loglik \n",
    "    loss.backward()\n",
    "    sgd.step()\n",
    "    toc = time.time()\n",
    "    \n",
    "    loglik = mle(seq, model)\n",
    "    \n",
    "    print(f\"Iter-{i}: log-likelihood = {float(loglik):.4f}; time cost = {toc-tic:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tppcourse",
   "language": "python",
   "name": "tppcourse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
